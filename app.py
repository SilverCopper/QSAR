"""
åŸºäºé¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®šé‡æ„æ•ˆå…³ç³»é¢„æµ‹å¹³å°

ä¸€ä¸ªé›†æˆå¤šç§åˆ†å­åµŒå…¥æ–¹æ³•å’Œæœºå™¨å­¦ä¹ ç®—æ³•çš„QSARé¢„æµ‹å¹³å°
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
import requests
import json

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="QSARæ·±åº¦å­¦ä¹ é¢„æµ‹å¹³å°",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¯¼å…¥å·¥å…·æ¨¡å—
from utils.embedding_utils import extract_embeddings, get_embedding_info
from utils.model_utils import ModelTrainer, plot_confusion_matrix, plot_roc_curve, plot_feature_importance
from utils.data_utils import (
    load_data, display_data_summary, validate_smiles, calculate_molecular_descriptors,
    plot_data_distribution, plot_correlation_matrix, plot_molecular_property_distribution,
    plot_lipinski_analysis, preprocess_data
)
from utils.file_utils import (
    create_project_directory, save_project_metadata, load_project_metadata,
    get_existing_projects, save_embeddings, load_embeddings, save_model_results,
    load_model_results, get_available_datasets
)

warnings.filterwarnings("ignore")

def main():
    """ä¸»å‡½æ•°"""
    
    st.markdown("""
    <style>
    .main-title {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3rem;
        font-weight: 700;
        text-align: center;
        margin-bottom: 2rem;
    }
    .feature-card {
        background: white;
        border-radius: 15px;
        padding: 2rem;
        margin: 1rem 0;
        box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        border-left: 5px solid #667eea;
        transition: all 0.3s ease;
        cursor: pointer;
    }
    .feature-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 20px 40px rgba(0,0,0,0.15);
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        padding: 1.5rem;
        text-align: center;
        margin: 0.5rem;
    }
    .nav-button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 0.5rem 1rem;
        margin: 0.2rem;
        cursor: pointer;
        transition: all 0.3s ease;
    }
    .nav-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
    .user-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        margin: 0.2rem;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ– session state
    if 'logged_in' not in st.session_state:
        st.session_state.logged_in = False
    if 'username' not in st.session_state:
        st.session_state.username = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 'login' if not st.session_state.logged_in else 'home'
    
    # æ£€æŸ¥ç™»å½•çŠ¶æ€
    if not st.session_state.logged_in:
        from utils.auth_utils import render_login_page
        render_login_page()
        return
    
    # é¡¶éƒ¨ç”¨æˆ·ä¿¡æ¯å’Œå¯¼èˆªæ  - è°ƒæ•´å¯¹é½
    col_user, col1, col2, col3, col4, col5, col6, col_logout = st.columns([1.2, 1, 1, 1, 1, 1, 1, 1])
    
    with col_user:
        # ä½¿ç”¨ä¸æŒ‰é’®å®Œå…¨ç›¸åŒçš„æ ·å¼å’Œé«˜åº¦
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 25px;
            margin: 0.2rem;
            text-align: center;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 14px;
            font-weight: 400;
            line-height: 1.6;
            border: 1px solid transparent;
            box-sizing: border-box;
        ">ğŸ‘¤ {st.session_state.username}</div>
        """, unsafe_allow_html=True)
    
    with col1:
        if st.button("ğŸ  é¦–é¡µ", key="nav_home"):
            st.session_state.current_page = 'home'
            st.rerun()
    with col2:
        if st.button("ğŸ“Š æ•°æ®æ¢ç´¢", key="nav_data"):
            st.session_state.current_page = 'data_exploration'
            st.rerun()
    with col3:
        if st.button("ğŸ§  æ™ºèƒ½å»ºæ¨¡", key="nav_smart_modeling"):
            st.session_state.current_page = 'smart_modeling'
            st.rerun()
    with col4:
        if st.button("ğŸ“ é¡¹ç›®ç®¡ç†", key="nav_projects"):
            st.session_state.current_page = 'project_viewer'
            st.rerun()
    with col5:
        if st.button("ğŸ“š æ–‡çŒ®æŒ–æ˜", key="nav_literature"):
            st.session_state.current_page = 'literature_mining'
            st.rerun()
    
    with col6:
        if st.button("ğŸ¤– AIæ‘˜è¦", key="nav_ai_summary"):
            st.session_state.current_page = 'ai_summary'
            st.rerun()
    
    with col_logout:
        if st.button("ğŸšª ç™»å‡º", key="nav_logout"):
            from utils.auth_utils import logout_user
            logout_user()
    
    st.markdown("---")
    
    # è·¯ç”±åˆ°å¯¹åº”é¡µé¢
    if st.session_state.current_page == 'home':
        render_homepage()
    elif st.session_state.current_page == 'data_exploration':
        render_data_exploration()
    elif st.session_state.current_page == 'smart_modeling':
        render_smart_modeling()
    elif st.session_state.current_page == 'project_viewer':
        render_project_viewer()
    elif st.session_state.current_page == 'literature_mining':
        render_literature_mining()
    elif st.session_state.current_page == 'ai_summary':
        render_ai_summary()

def render_homepage():
    """æ¸²æŸ“é¦–é¡µ"""
    st.markdown('<h1 class="main-title">ğŸ§¬ QSARæ·±åº¦å­¦ä¹ é¢„æµ‹å¹³å°</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #7f8c8d; margin-bottom: 3rem;">åŸºäºé¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å®šé‡æ„æ•ˆå…³ç³»é¢„æµ‹</p>', unsafe_allow_html=True)
    
    # å¹³å°ç‰¹æ€§å±•ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="feature-card">
            <div style="font-size: 3rem; text-align: center; margin-bottom: 1rem;">ğŸ“Š</div>
            <div style="color: #667eea; font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">æ•°æ®æ¢ç´¢åˆ†æ</div>
            <div style="text-align: center; line-height: 1.6;">
                æ™ºèƒ½æ•°æ®é¢„å¤„ç†ã€å¯è§†åŒ–åˆ†æ<br/>
                åˆ†å­æè¿°ç¬¦è®¡ç®—ã€SMILESéªŒè¯<br/>
                Lipinskiè§„åˆ™åˆ†æ
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <div style="font-size: 3rem; text-align: center; margin-bottom: 1rem;">ğŸ”¬</div>
            <div style="color: #667eea; font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">å¤šæ¨¡æ€åµŒå…¥æå–</div>
            <div style="text-align: center; line-height: 1.6;">
                RDKitåˆ†å­æŒ‡çº¹ã€ChemBERTa<br/>
                SMILES Transformer<br/>
                å¤šç»´åº¦åˆ†å­è¡¨ç¤ºå­¦ä¹ 
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="feature-card">
            <div style="font-size: 3rem; text-align: center; margin-bottom: 1rem;">ğŸ§ª</div>
            <div style="color: #667eea; font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">å®æ—¶é¢„æµ‹å»ºæ¨¡</div>
            <div style="text-align: center; line-height: 1.6;">
                LightGBMã€XGBoostã€éšæœºæ£®æ—<br/>
                è®­ç»ƒå®Œæˆç«‹å³é¢„æµ‹<br/>
                æ”¯æŒå•ä¸ªå’Œæ‰¹é‡é¢„æµ‹
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="feature-card">
            <div style="font-size: 3rem; text-align: center; margin-bottom: 1rem;">ğŸ§ </div>
            <div style="color: #667eea; font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem; text-align: center;">AIæ™ºèƒ½æ‘˜è¦</div>
            <div style="text-align: center; line-height: 1.6;">
                æ–‡çŒ®æ‘˜è¦æ™ºèƒ½åˆ†æ<br/>
                æ ¸å¿ƒè¦ç‚¹è‡ªåŠ¨æç‚¼<br/>
                ä¸­æ–‡è¾“å‡ºå…³é”®ä¿¡æ¯
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    # ä¸ªäººç»Ÿè®¡ä¿¡æ¯
    st.markdown("### ğŸ“ˆ ä¸ªäººç»Ÿè®¡")
    
    # è·å–ç”¨æˆ·æ•°æ®
    from utils.user_data_utils import UserDataManager
    user_data = UserDataManager(st.session_state.username)
    user_projects = user_data.get_user_projects()
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 2rem; font-weight: 700; margin-bottom: 0.5rem;">{len(user_projects['embeddings'])}</div>
            <div style="font-size: 0.9rem; opacity: 0.9;">åµŒå…¥é¡¹ç›®</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 2rem; font-weight: 700; margin-bottom: 0.5rem;">{len(user_projects['models'])}</div>
            <div style="font-size: 0.9rem; opacity: 0.9;">æ¨¡å‹é¡¹ç›®</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 2rem; font-weight: 700; margin-bottom: 0.5rem;">{len(user_projects['uploads'])}</div>
            <div style="font-size: 0.9rem; opacity: 0.9;">ä¸Šä¼ æ–‡ä»¶</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        total_size = sum(f['size'] for f in user_projects['uploads'])
        st.markdown(f"""
        <div class="metric-card">
            <div style="font-size: 2rem; font-weight: 700; margin-bottom: 0.5rem;">{total_size/1024/1024:.1f}</div>
            <div style="font-size: 0.9rem; opacity: 0.9;">å­˜å‚¨(MB)</div>
        </div>
        """, unsafe_allow_html=True)
    
    # å¿«é€Ÿå¼€å§‹æŒ‡å—
    st.markdown("### ğŸš€ å¿«é€Ÿå¼€å§‹")
    st.info("""
    **æ™ºèƒ½å»ºæ¨¡æµç¨‹ï¼š**
    1. **æ•°æ®æ¢ç´¢** â†’ ä¸Šä¼ æ•°æ®é›†ï¼Œè¿›è¡Œåˆ†å­æ€§è´¨åˆ†æå’Œå¯è§†åŒ–
    2. **æ™ºèƒ½å»ºæ¨¡** â†’ ä¸€ç«™å¼å®Œæˆåˆ†å­åµŒå…¥æå–å’Œæœºå™¨å­¦ä¹ å»ºæ¨¡
    3. **é¡¹ç›®ç®¡ç†** â†’ æŸ¥çœ‹å’Œç®¡ç†ä¸ªäººçš„åµŒå…¥é¡¹ç›®ã€æ¨¡å‹å’Œæ–‡ä»¶
    4. **æ–‡çŒ®æŒ–æ˜** â†’ æœç´¢ç›¸å…³ç ”ç©¶æ–‡çŒ®ï¼Œæ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥
    5. **AIæ‘˜è¦** â†’ æ™ºèƒ½åˆ†ææ–‡çŒ®æ‘˜è¦ï¼Œæç‚¼æ ¸å¿ƒè¦ç‚¹å’Œåˆ›æ–°ç‚¹
    
    ğŸ’¡ **æç¤º**: æ–°ç”¨æˆ·å¯ä»¥ä»"æ™ºèƒ½å»ºæ¨¡"å¼€å§‹ï¼Œä½“éªŒå®Œæ•´çš„QSARå»ºæ¨¡æµç¨‹
    """)

def render_data_exploration():
    """æ¸²æŸ“æ•°æ®æ¢ç´¢é¡µé¢"""
    st.title("ğŸ“Š æ•°æ®æ¢ç´¢åˆ†æ")
    
    # æ•°æ®æºé€‰æ‹©
    data_source = st.radio(
        "é€‰æ‹©æ•°æ®æº",
        ["ä½¿ç”¨ç°æœ‰æ•°æ®é›†", "ä¸Šä¼ æ–°æ•°æ®"],
        horizontal=True
    )
    
    data = None
    
    if data_source == "ä½¿ç”¨ç°æœ‰æ•°æ®é›†":
        datasets = get_available_datasets()
        if datasets:
            dataset_names = [os.path.basename(f) for f in datasets]
            selected_dataset = st.selectbox("é€‰æ‹©æ•°æ®é›†", dataset_names)
            
            if selected_dataset:
                dataset_path = next(f for f in datasets if os.path.basename(f) == selected_dataset)
                data = load_data(dataset_path)
        else:
            st.warning("dataç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶")
            
    else:
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ•°æ®æ–‡ä»¶", 
            type=['csv', 'xlsx', 'xls'],
            help="æ”¯æŒCSVå’ŒExcelæ ¼å¼"
        )
        
        if uploaded_file:
            if uploaded_file.name.endswith('.csv'):
                data = pd.read_csv(uploaded_file)
            else:
                data = pd.read_excel(uploaded_file)
    
    # æ•°æ®åˆ†æ
    if data is not None:
        # åŸºæœ¬ä¿¡æ¯å±•ç¤º
        display_data_summary(data)
        
        # æ•°æ®é¢„è§ˆ
        st.subheader("ğŸ” æ•°æ®é¢„è§ˆ")
        st.dataframe(data.head(10), use_container_width=True)
        
        # SMILESåˆ—æ£€æµ‹å’ŒéªŒè¯
        st.subheader("ğŸ§ª SMILESåˆ†æ")
        
        smiles_columns = [col for col in data.columns if 'smiles' in col.lower()]
        if smiles_columns:
            smiles_col = st.selectbox("é€‰æ‹©SMILESåˆ—", smiles_columns)
            
            if smiles_col:
                # SMILESéªŒè¯
                smiles_list = data[smiles_col].dropna().tolist()
                validation_result = validate_smiles(smiles_list)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»åˆ†å­æ•°", validation_result['total'])
                with col2:
                    st.metric("æœ‰æ•ˆåˆ†å­æ•°", validation_result['valid'])
                with col3:
                    st.metric("æœ‰æ•ˆç‡", f"{validation_result['valid_ratio']:.1%}")
                
                if validation_result['invalid'] > 0:
                    with st.expander(f"æŸ¥çœ‹æ— æ•ˆSMILES ({validation_result['invalid']}ä¸ª)"):
                        invalid_df = pd.DataFrame(validation_result['invalid_smiles'], 
                                                columns=['ç´¢å¼•', 'SMILES'])
                        st.dataframe(invalid_df)
                
                # åˆ†å­æè¿°ç¬¦è®¡ç®—
                if st.button("è®¡ç®—åˆ†å­æè¿°ç¬¦"):
                    with st.spinner("æ­£åœ¨è®¡ç®—åˆ†å­æè¿°ç¬¦..."):
                        descriptors_df = calculate_molecular_descriptors(smiles_list)
                        
                        st.subheader("ğŸ“ˆ åˆ†å­æ€§è´¨åˆ†å¸ƒ")
                        fig = plot_molecular_property_distribution(descriptors_df)
                        st.pyplot(fig)
                        
                        st.subheader("âš–ï¸ Lipinskiè§„åˆ™åˆ†æ")
                        fig = plot_lipinski_analysis(descriptors_df)
                        st.pyplot(fig)
                        
                        # æè¿°ç¬¦ç»Ÿè®¡
                        st.subheader("ğŸ“‹ æè¿°ç¬¦ç»Ÿè®¡")
                        st.dataframe(descriptors_df.describe(), use_container_width=True)
        else:
            st.warning("æœªæ‰¾åˆ°SMILESåˆ—ï¼Œè¯·ç¡®ä¿æ•°æ®ä¸­åŒ…å«åä¸º'smiles'æˆ–ç±»ä¼¼çš„åˆ—")

def render_smart_modeling():
    """æ¸²æŸ“æ™ºèƒ½å»ºæ¨¡é¡µé¢ï¼ˆåˆå¹¶åµŒå…¥æå–å’Œæ¨¡å‹è®­ç»ƒï¼‰"""
    st.title("ğŸ§  æ™ºèƒ½å»ºæ¨¡å¹³å°")
    st.markdown("**ä¸€ç«™å¼åˆ†å­åµŒå…¥æå–ä¸æœºå™¨å­¦ä¹ å»ºæ¨¡**")
    
    # è·å–ç”¨æˆ·æ•°æ®ç®¡ç†å™¨
    from utils.user_data_utils import UserDataManager
    user_data = UserDataManager(st.session_state.username)
    
    # å»ºæ¨¡æµç¨‹é€‰æ‹©
    modeling_mode = st.radio(
        "é€‰æ‹©å»ºæ¨¡æ–¹å¼",
        ["æ–°å»ºé¡¹ç›®", "ä½¿ç”¨å·²æœ‰åµŒå…¥"],
        horizontal=True
    )
    
    if modeling_mode == "æ–°å»ºé¡¹ç›®":
        render_new_modeling_project(user_data)
    else:
        render_existing_embedding_modeling(user_data)

def render_new_modeling_project(user_data):
    """æ¸²æŸ“æ–°å»ºå»ºæ¨¡é¡¹ç›®"""
    st.subheader("ğŸ“Š æ­¥éª¤1: æ•°æ®å‡†å¤‡")
    
    # æ•°æ®è¾“å…¥æ–¹å¼
    input_method = st.radio("æ•°æ®è¾“å…¥æ–¹å¼", ["ä¸Šä¼ æ–‡ä»¶", "æ‰‹åŠ¨è¾“å…¥", "ä½¿ç”¨å·²ä¸Šä¼ æ–‡ä»¶"])
    
    data_df = None
    smiles_data = []
    labels = None
    
    if input_method == "ä¸Šä¼ æ–‡ä»¶":
        uploaded_file = st.file_uploader("ä¸Šä¼ åŒ…å«SMILESå’Œæ ‡ç­¾çš„CSVæ–‡ä»¶", type=['csv'])
        if uploaded_file:
            # ä¿å­˜æ–‡ä»¶åˆ°ç”¨æˆ·ç›®å½•
            filepath, filename = user_data.save_uploaded_file(uploaded_file)
            
            data_df = pd.read_csv(uploaded_file)
            st.write("æ–‡ä»¶é¢„è§ˆï¼š")
            st.dataframe(data_df.head())
            
    elif input_method == "ä½¿ç”¨å·²ä¸Šä¼ æ–‡ä»¶":
        projects = user_data.get_user_projects()
        if projects['uploads']:
            upload_files = [f['filename'] for f in projects['uploads']]
            selected_file = st.selectbox("é€‰æ‹©æ–‡ä»¶", upload_files)
            
            if selected_file:
                # æ‰¾åˆ°æ–‡ä»¶è·¯å¾„
                file_info = next(f for f in projects['uploads'] if f['filename'] == selected_file)
                data_df = pd.read_csv(file_info['filepath'])
                st.write("æ–‡ä»¶é¢„è§ˆï¼š")
                st.dataframe(data_df.head())
        else:
            st.info("æ²¡æœ‰å·²ä¸Šä¼ çš„æ–‡ä»¶")
            
    else:  # æ‰‹åŠ¨è¾“å…¥
        col1, col2 = st.columns(2)
        with col1:
            smiles_input = st.text_area("è¾“å…¥SMILESï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=150)
            if smiles_input:
                smiles_data = [s.strip() for s in smiles_input.split('\n') if s.strip()]
        
        with col2:
            labels_input = st.text_area("è¾“å…¥å¯¹åº”æ ‡ç­¾ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰", height=150)
            if labels_input:
                labels = [float(l.strip()) if l.strip().replace('.','').replace('-','').isdigit() 
                         else l.strip() for l in labels_input.split('\n') if l.strip()]
        
        if smiles_data and labels and len(smiles_data) == len(labels):
            data_df = pd.DataFrame({'SMILES': smiles_data, 'Label': labels})
            st.write("æ•°æ®é¢„è§ˆï¼š")
            st.dataframe(data_df.head())
    
    if data_df is not None:
        # åˆ—é€‰æ‹©
        st.subheader("ğŸ¯ æ­¥éª¤2: é€‰æ‹©åˆ—")
        
        smiles_col = st.selectbox("é€‰æ‹©SMILESåˆ—", data_df.columns.tolist())
        label_col = st.selectbox("é€‰æ‹©æ ‡ç­¾åˆ—", [col for col in data_df.columns if col != smiles_col])
        
        if smiles_col and label_col:
            smiles_data = data_df[smiles_col].dropna().tolist()
            labels = data_df[label_col].dropna().tolist()
            
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            min_len = min(len(smiles_data), len(labels))
            smiles_data = smiles_data[:min_len]
            labels = labels[:min_len]
            
            st.success(f"å‡†å¤‡äº† {len(smiles_data)} ä¸ªæ ·æœ¬ç”¨äºå»ºæ¨¡")
            
            # åµŒå…¥æå–
            st.subheader("ğŸ”¬ æ­¥éª¤3: åˆ†å­åµŒå…¥æå–")
            
            embedding_method = st.selectbox(
                "é€‰æ‹©åµŒå…¥æ–¹æ³•",
                ["RDKit æŒ‡çº¹", "ChemBERTa åµŒå…¥", "SMILES Transformer åµŒå…¥"]
            )
            
            # é¡¹ç›®å‘½å
            project_name = st.text_input(
                "é¡¹ç›®åç§°ï¼ˆå¯é€‰ï¼‰",
                placeholder="ä¸ºæ‚¨çš„åµŒå…¥é¡¹ç›®å‘½åï¼Œç•™ç©ºå°†ä½¿ç”¨é»˜è®¤åç§°",
                help="è‡ªå®šä¹‰é¡¹ç›®åç§°ä¾¿äºåç»­ç®¡ç†å’Œè¯†åˆ«"
            )
            
            if st.button("æå–åµŒå…¥å‘é‡"):
                with st.spinner(f"ä½¿ç”¨ {embedding_method} æå–åµŒå…¥å‘é‡..."):
                    from utils.embedding_utils import extract_embeddings
                    
                    try:
                        embeddings = extract_embeddings(smiles_data, embedding_method)
                        
                        if embeddings is not None:
                            st.success(f"æˆåŠŸæå–äº† {len(embeddings)} ä¸ªåˆ†å­çš„åµŒå…¥å‘é‡")
                            st.write(f"åµŒå…¥å‘é‡ç»´åº¦: {embeddings.shape}")
                            
                            # ä¿å­˜åµŒå…¥ç»“æœ
                            project_id, project_folder = user_data.save_embedding_result(
                                smiles_data, embeddings, embedding_method, labels,
                                metadata={'original_data_cols': [smiles_col, label_col]},
                                project_name=project_name
                            )
                            
                            st.success(f"åµŒå…¥ç»“æœå·²ä¿å­˜ï¼Œé¡¹ç›®ID: {project_id}")
                            
                            # ä¿å­˜åˆ°session stateä»¥ä¾¿è®­ç»ƒä½¿ç”¨
                            st.session_state.current_embeddings = embeddings
                            st.session_state.current_labels = labels
                            st.session_state.current_method = embedding_method
                            st.session_state.embedding_ready = True
                            
                            st.info("âœ… åµŒå…¥æå–å®Œæˆï¼ç°åœ¨å¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚")
                            
                    except Exception as e:
                        st.error(f"åµŒå…¥æå–å¤±è´¥: {str(e)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‡†å¤‡å¥½çš„åµŒå…¥æ•°æ®ï¼Œå¦‚æœæœ‰åˆ™æ˜¾ç¤ºè®­ç»ƒç•Œé¢
            if st.session_state.get('embedding_ready', False):
                embeddings = st.session_state.current_embeddings
                labels = st.session_state.current_labels
                method_name = st.session_state.current_method
                
                st.markdown("---")
                render_model_training_section(embeddings, labels, user_data, method_name)

            # å¦‚æœè®­ç»ƒç»“æœå­˜åœ¨ï¼Œåˆ™æ˜¾ç¤ºå®ƒ
            if 'last_training_result' in st.session_state:
                st.markdown("---")
                render_training_results(st.session_state.last_training_result)

            # æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œç‹¬ç«‹æ¸²æŸ“é¢„æµ‹ç•Œé¢
            if st.session_state.get('model_trained', False):
                render_realtime_prediction_interface()

def render_existing_embedding_modeling(user_data):
    """ä½¿ç”¨å·²æœ‰åµŒå…¥è¿›è¡Œå»ºæ¨¡"""
    st.subheader("ğŸ“ é€‰æ‹©å·²æœ‰åµŒå…¥é¡¹ç›®")
    
    embedding_projects = user_data.get_embedding_projects_for_modeling()
    
    if not embedding_projects:
        st.info("æ²¡æœ‰å¯ç”¨äºå»ºæ¨¡çš„åµŒå…¥é¡¹ç›®ã€‚è¯·å…ˆåˆ›å»ºåŒ…å«æ ‡ç­¾çš„åµŒå…¥é¡¹ç›®ã€‚")
        return
    
    # é¡¹ç›®é€‰æ‹©
    project_options = [f"{p['method']} - {p['num_molecules']}åˆ†å­ - {p['created_at']}" 
                      for p in embedding_projects]
    selected_project_idx = st.selectbox("é€‰æ‹©åµŒå…¥é¡¹ç›®", range(len(project_options)), 
                                       format_func=lambda x: project_options[x])
    
    if selected_project_idx is not None:
        selected_project = embedding_projects[selected_project_idx]
        
        # åŠ è½½å»ºæ¨¡æ•°æ®
        modeling_data = user_data.load_modeling_data(selected_project['project_id'])
        
        if modeling_data is not None:
            st.write("æ•°æ®é¢„è§ˆï¼š")
            st.dataframe(modeling_data.head())
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            feature_cols = [col for col in modeling_data.columns if col not in ['SMILES', 'Label']]
            X = modeling_data[feature_cols].values
            y = modeling_data['Label'].values
            
            st.success(f"åŠ è½½äº† {len(X)} ä¸ªæ ·æœ¬ï¼Œç‰¹å¾ç»´åº¦: {X.shape[1]}")
            
            # è¿›å…¥æ¨¡å‹è®­ç»ƒ
            render_model_training_section(X, y, user_data, selected_project['method'])

            # å¦‚æœè®­ç»ƒç»“æœå­˜åœ¨ï¼Œåˆ™æ˜¾ç¤ºå®ƒ
            if 'last_training_result' in st.session_state:
                st.markdown("---")
                render_training_results(st.session_state.last_training_result)

            # æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œç‹¬ç«‹æ¸²æŸ“é¢„æµ‹ç•Œé¢
            if st.session_state.get('model_trained', False):
                render_realtime_prediction_interface()

def render_model_training_section(X, y, user_data, method_name):
    """æ¸²æŸ“æ¨¡å‹è®­ç»ƒéƒ¨åˆ†"""
    st.subheader("ğŸ¤– æ­¥éª¤4: æœºå™¨å­¦ä¹ å»ºæ¨¡")
    
    # ä»»åŠ¡ç±»å‹æ£€æµ‹
    unique_labels = len(np.unique(y))
    if unique_labels <= 10:
        task_type = st.selectbox("ä»»åŠ¡ç±»å‹", ["åˆ†ç±»", "å›å½’"], index=0)
    else:
        task_type = st.selectbox("ä»»åŠ¡ç±»å‹", ["å›å½’", "åˆ†ç±»"], index=0)
    
    # ç®—æ³•é€‰æ‹©
    algorithm = st.selectbox(
        "é€‰æ‹©ç®—æ³•",
        ["LightGBM", "XGBoost", "éšæœºæ£®æ—", "æ”¯æŒå‘é‡æœº"]
    )
    
    # è®­ç»ƒå‚æ•°
    col1, col2 = st.columns(2)
    with col1:
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.4, 0.2, 0.05)
    with col2:
        random_state = st.number_input("éšæœºç§å­", value=42, min_value=0)
    
    # å¼€å§‹è®­ç»ƒ
    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", use_container_width=True):
        with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
            from utils.model_utils import train_model
            
            try:
                # è®­ç»ƒæ¨¡å‹
                result = train_model(X, y, algorithm, task_type, test_size)
                
                if result:
                    st.success("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
                    
                    # å°†è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå®Œæ•´ç»“æœä¿å­˜åˆ°session state
                    st.session_state.last_training_result = result
                    st.session_state.trained_model = result['model']
                    st.session_state.model_algorithm = algorithm
                    st.session_state.model_task_type = task_type
                    st.session_state.embedding_method = method_name
                    st.session_state.model_metrics = result['metrics']
                    st.session_state.model_trained = True
                    
                    # é‡æ–°è¿è¡Œè„šæœ¬ä»¥æ˜¾ç¤ºç»“æœå’Œé¢„æµ‹ç•Œé¢
                    st.rerun()
                    
            except Exception as e:
                st.error(f"è®­ç»ƒå¤±è´¥: {str(e)}")
                st.error("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå‚æ•°è®¾ç½®")
    
    # æ–¹æ³•è¯´æ˜
    with st.expander("ğŸ’¡ æ–¹æ³•è¯´æ˜"):
        st.info(f"""
        **åµŒå…¥æ–¹æ³•**: {method_name}
        **ç®—æ³•è¯´æ˜**:
        - **LightGBM**: åŸºäºæ¢¯åº¦æå‡çš„é«˜æ•ˆç®—æ³•ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®
        - **XGBoost**: ç»å…¸çš„æ¢¯åº¦æå‡ç®—æ³•ï¼Œæ€§èƒ½ä¼˜ç§€
        - **éšæœºæ£®æ—**: é›†æˆå­¦ä¹ ç®—æ³•ï¼Œå¯¹è¿‡æ‹Ÿåˆä¸æ•æ„Ÿ
        - **æ”¯æŒå‘é‡æœº**: é€‚åˆå°æ ·æœ¬æ•°æ®ï¼Œæœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›
        """)

def render_training_results(result):
    """æ¸²æŸ“æ¨¡å‹è®­ç»ƒçš„è¯„ä¼°ç»“æœ"""
    st.subheader("ğŸ“ˆ æ¨¡å‹è¯„ä¼°ç»“æœ")
    
    col1, col2 = st.columns(2)
    with col1:
        st.write("**å…³é”®æŒ‡æ ‡**:")
        for metric, value in result['metrics'].items():
            st.metric(metric, f"{value:.4f}")
    
    with col2:
        if 'confusion_matrix' in result:
            st.write("**æ··æ·†çŸ©é˜µ**:")
            st.dataframe(result['confusion_matrix'])

    # ROCæ›²çº¿ï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
    if result.get('task_type') == "åˆ†ç±»" and 'roc_data' in result:
        st.subheader("ğŸ“ˆ ROCæ›²çº¿")
        
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots(figsize=(8, 6), dpi=150)
        
        roc_data = result['roc_data']
        ax.plot(roc_data['fpr'], roc_data['tpr'], color='blue', lw=2, 
               label=f'ROC Curve (AUC = {roc_data["roc_auc"]:.3f})')
        ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontsize=12)
        ax.set_ylabel('True Positive Rate', fontsize=12)
        ax.set_title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')
        ax.legend(loc="lower right", fontsize=11)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig, use_container_width=True)
        plt.close()

def render_realtime_prediction_interface():
    """æ¸²æŸ“å®æ—¶é¢„æµ‹ç•Œé¢"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    if 'trained_model' not in st.session_state:
        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    st.subheader("ğŸ§ª å®æ—¶åˆ†å­é¢„æµ‹")
    st.info("ğŸ’¡ æ¨¡å‹å·²è®­ç»ƒå®Œæˆï¼Œæ‚¨å¯ä»¥ç›´æ¥è¾“å…¥SMILESè¿›è¡Œé¢„æµ‹")
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    with st.expander("ğŸ“‹ å½“å‰æ¨¡å‹ä¿¡æ¯"):
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**ç®—æ³•**: {st.session_state.get('model_algorithm', 'Unknown')}")
            st.write(f"**ä»»åŠ¡ç±»å‹**: {st.session_state.get('model_task_type', 'Unknown')}")
            st.write(f"**åµŒå…¥æ–¹æ³•**: {st.session_state.get('embedding_method', 'Unknown')}")
        with col2:
            st.write("**æ¨¡å‹æ€§èƒ½**:")
            metrics = st.session_state.get('model_metrics', {})
            for metric, value in metrics.items():
                if isinstance(value, (int, float)):
                    st.write(f"- {metric}: {value:.4f}")
                else:
                    st.write(f"- {metric}: {value}")
    
    # é¢„æµ‹è¾“å…¥é€‰é¡¹
    prediction_mode = st.radio(
        "é€‰æ‹©é¢„æµ‹æ–¹å¼",
        ["å•ä¸ªåˆ†å­é¢„æµ‹", "æ‰¹é‡åˆ†å­é¢„æµ‹"],
        horizontal=True,
        key="prediction_mode_radio"
    )
    
    if prediction_mode == "å•ä¸ªåˆ†å­é¢„æµ‹":
        render_single_molecule_prediction()
    else:
        render_batch_molecule_prediction()

def render_single_molecule_prediction():
    """æ¸²æŸ“å•ä¸ªåˆ†å­é¢„æµ‹ç•Œé¢"""
    st.write("### å•ä¸ªåˆ†å­é¢„æµ‹")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if 'trained_model' not in st.session_state:
        st.error("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # ä½¿ç”¨è¡¨å•æ¥å¤„ç†è¾“å…¥å’Œæäº¤
    with st.form(key='single_prediction_form'):
        smiles_input = st.text_input(
            "è¾“å…¥SMILESå­—ç¬¦ä¸²", 
            value=st.session_state.get('prediction_smiles', ''),
            placeholder="ä¾‹å¦‚: CCO (ä¹™é†‡)",
            key="prediction_input"
        )
        
        submitted = st.form_submit_button("ğŸ” å¼€å§‹é¢„æµ‹", type="primary", use_container_width=True)
        
        if submitted:
            if smiles_input.strip():
                # æ‰§è¡Œé¢„æµ‹
                result = perform_prediction(smiles_input.strip())
                if result:
                    st.session_state.prediction_result = result
                else:
                    # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œæ¸…é™¤æ—§ç»“æœ
                    if 'prediction_result' in st.session_state:
                        del st.session_state.prediction_result
            else:
                st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„SMILESå­—ç¬¦ä¸²")

    # åœ¨è¡¨å•å¤–éƒ¨æ˜¾ç¤ºç»“æœï¼Œé¿å…ç»“æœåœ¨ä¸‹æ¬¡æäº¤æ—¶æ¶ˆå¤±
    if 'prediction_result' in st.session_state:
        display_prediction_result(st.session_state.prediction_result)

def render_batch_molecule_prediction():
    """æ¸²æŸ“æ‰¹é‡åˆ†å­é¢„æµ‹ç•Œé¢"""
    st.write("### æ‰¹é‡åˆ†å­é¢„æµ‹")
    
    if 'trained_model' not in st.session_state:
        st.error("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    # ä½¿ç”¨è¡¨å•æ¥å¤„ç†è¾“å…¥å’Œæäº¤
    with st.form(key='batch_prediction_form'):
        smiles_batch = st.text_area(
            "è¾“å…¥å¤šä¸ªSMILESï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            value=st.session_state.get('batch_smiles', ''),
            placeholder="CCO\nC1=CC=CC=C1\n...",
            height=150,
            help="æ¯è¡Œè¾“å…¥ä¸€ä¸ªSMILESå­—ç¬¦ä¸²ï¼Œæœ€å¤šæ”¯æŒ50ä¸ªåˆ†å­",
            key="batch_input"
        )
        
        submitted = st.form_submit_button("ğŸ” æ‰¹é‡é¢„æµ‹", type="primary", use_container_width=True)
        
        if submitted:
            if smiles_batch.strip():
                smiles_list = [s.strip() for s in smiles_batch.strip().split('\n') if s.strip()]
                
                if not smiles_list:
                    st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„SMILESå­—ç¬¦ä¸²")
                elif len(smiles_list) > 50:
                    st.warning("âš ï¸ æ‰¹é‡é¢„æµ‹æœ€å¤šæ”¯æŒ50ä¸ªåˆ†å­ï¼Œå·²æˆªå–å‰50ä¸ª")
                    smiles_list = smiles_list[:50]
                
                if smiles_list:
                    # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
                    result = perform_batch_prediction(smiles_list)
                    if result:
                        st.session_state.batch_prediction_result = result
                    else:
                        # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œæ¸…é™¤æ—§ç»“æœ
                        if 'batch_prediction_result' in st.session_state:
                            del st.session_state.batch_prediction_result
            else:
                st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„SMILESå­—ç¬¦ä¸²")

    # åœ¨è¡¨å•å¤–éƒ¨æ˜¾ç¤ºç»“æœ
    if 'batch_prediction_result' in st.session_state:
        display_batch_prediction_result(st.session_state.batch_prediction_result)

def perform_prediction(smiles):
    """æ‰§è¡Œåˆ†å­é¢„æµ‹
    
    å®Œæ•´æµç¨‹ï¼š
    1. ä»session_stateè·å–è®­ç»ƒå¥½çš„æ¨¡å‹
    2. ä»session_stateè·å–è®­ç»ƒæ—¶ä½¿ç”¨çš„åµŒå…¥æ–¹æ³•
    3. ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ–¹æ³•ä¸ºæ–°åˆ†å­ç”ŸæˆåµŒå…¥
    4. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹åµŒå…¥è¿›è¡Œé¢„æµ‹
    """
    try:
        # æ­¥éª¤1: è·å–è®­ç»ƒå¥½çš„æ¨¡å‹
        trained_model = st.session_state.get('trained_model')
        if trained_model is None:
            st.error("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # æ­¥éª¤2: è·å–è®­ç»ƒæ—¶ä½¿ç”¨çš„åµŒå…¥æ–¹æ³•
        embedding_method = st.session_state.get('embedding_method')
        if not embedding_method:
            st.error("âŒ æ²¡æœ‰æ‰¾åˆ°åµŒå…¥æ–¹æ³•ä¿¡æ¯ï¼Œè¯·é‡æ–°è®­ç»ƒæ¨¡å‹")
            return None
        
        # è·å–å…¶ä»–æ¨¡å‹ä¿¡æ¯
        task_type = st.session_state.get('model_task_type', '')
        algorithm = st.session_state.get('model_algorithm', 'Unknown')
        
        with st.spinner("æ­£åœ¨æå–åˆ†å­åµŒå…¥å¹¶é¢„æµ‹..."):
            # æ­¥éª¤3: ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„åµŒå…¥æ–¹æ³•ä¸ºæ–°åˆ†å­ç”ŸæˆåµŒå…¥
            from utils.embedding_utils import extract_embeddings
            embeddings = extract_embeddings([smiles], embedding_method)
            
            if embeddings is None or len(embeddings) == 0:
                st.error("âŒ åˆ†å­åµŒå…¥æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥SMILESæ ¼å¼æ˜¯å¦æ­£ç¡®")
                return None
            
            # éªŒè¯åµŒå…¥ç»´åº¦æ˜¯å¦ä¸æ¨¡å‹æœŸæœ›ä¸€è‡´
            if hasattr(trained_model, 'n_features_in_'):
                expected_features = trained_model.n_features_in_
                actual_features = embeddings.shape[1]
                if expected_features != actual_features:
                    st.error(f"âŒ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼šæ¨¡å‹æœŸæœ›{expected_features}ç»´ï¼Œå®é™…{actual_features}ç»´")
                    return None
            
            # æ­¥éª¤4: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
            prediction = trained_model.predict(embeddings)[0]
            
            # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼‰
            probabilities = None
            confidence = None
            if task_type == "åˆ†ç±»" and hasattr(trained_model, 'predict_proba'):
                probabilities = trained_model.predict_proba(embeddings)[0]
                confidence = np.max(probabilities)
        
        # è¿”å›é¢„æµ‹ç»“æœ
        return {
            'smiles': smiles,
            'prediction': prediction,
            'probabilities': probabilities,
            'confidence': confidence,
            'task_type': task_type,
            'algorithm': algorithm,
            'embedding_method': embedding_method
        }
        
    except Exception as e:
        st.error(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def display_prediction_result(result):
    """æ˜¾ç¤ºå•ä¸ªåˆ†å­é¢„æµ‹ç»“æœ"""
    st.subheader("ğŸ’¡ é¢„æµ‹ç»“æœ")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        st.write("**è¾“å…¥åˆ†å­**:")
        st.code(result['smiles'], language='smiles')
        
        # æ˜¾ç¤ºåˆ†å­ç»“æ„
        from rdkit import Chem
        from rdkit.Chem import Draw
        mol = Chem.MolFromSmiles(result['smiles'])
        if mol:
            img = Draw.MolToImage(mol, size=(250, 200))
            st.image(img)

    with col2:
        st.metric("é¢„æµ‹æ ‡ç­¾", str(result['prediction']))
        if result.get('confidence'):
            st.metric("ç½®ä¿¡åº¦", f"{result['confidence']:.2%}")
        
        # å¯è§†åŒ–é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼‰
        if result.get('probabilities') is not None:
            st.write("**é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ**:")
            prob_df = pd.DataFrame({
                'ç±»åˆ«': [f'ç±»åˆ« {i}' for i in range(len(result['probabilities']))],
                'æ¦‚ç‡': result['probabilities']
            })
            st.dataframe(prob_df.style.format({'æ¦‚ç‡': '{:.2%}'}))

def perform_batch_prediction(smiles_list):
    """æ‰§è¡Œæ‰¹é‡åˆ†å­é¢„æµ‹
    
    å®Œæ•´æµç¨‹ï¼š
    1. ä»session_stateè·å–è®­ç»ƒå¥½çš„æ¨¡å‹
    2. ä»session_stateè·å–è®­ç»ƒæ—¶ä½¿ç”¨çš„åµŒå…¥æ–¹æ³•
    3. ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ–¹æ³•ä¸ºæ–°åˆ†å­åˆ—è¡¨ç”ŸæˆåµŒå…¥
    4. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹åµŒå…¥è¿›è¡Œæ‰¹é‡é¢„æµ‹
    """
    try:
        # æ­¥éª¤1: è·å–è®­ç»ƒå¥½çš„æ¨¡å‹
        trained_model = st.session_state.get('trained_model')
        if trained_model is None:
            st.error("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # æ­¥éª¤2: è·å–è®­ç»ƒæ—¶ä½¿ç”¨çš„åµŒå…¥æ–¹æ³•
        embedding_method = st.session_state.get('embedding_method')
        if not embedding_method:
            st.error("âŒ æ²¡æœ‰æ‰¾åˆ°åµŒå…¥æ–¹æ³•ä¿¡æ¯ï¼Œè¯·é‡æ–°è®­ç»ƒæ¨¡å‹")
            return None
        
        # è·å–å…¶ä»–æ¨¡å‹ä¿¡æ¯
        task_type = st.session_state.get('model_task_type', '')
        algorithm = st.session_state.get('model_algorithm', 'Unknown')
        
        # æ˜¾ç¤ºè¿›åº¦
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text(f"æ­£åœ¨æå– {len(smiles_list)} ä¸ªåˆ†å­çš„åµŒå…¥...")
        
        # æ­¥éª¤3: ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„åµŒå…¥æ–¹æ³•ä¸ºæ–°åˆ†å­åˆ—è¡¨ç”ŸæˆåµŒå…¥
        from utils.embedding_utils import extract_embeddings
        embeddings = extract_embeddings(smiles_list, embedding_method)
        
        if embeddings is None or len(embeddings) == 0:
            st.error("âŒ åˆ†å­åµŒå…¥æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥SMILESæ ¼å¼æ˜¯å¦æ­£ç¡®")
            return None
        
        # éªŒè¯åµŒå…¥ç»´åº¦æ˜¯å¦ä¸æ¨¡å‹æœŸæœ›ä¸€è‡´
        if hasattr(trained_model, 'n_features_in_'):
            expected_features = trained_model.n_features_in_
            actual_features = embeddings.shape[1]
            if expected_features != actual_features:
                st.error(f"âŒ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼šæ¨¡å‹æœŸæœ›{expected_features}ç»´ï¼Œå®é™…{actual_features}ç»´")
                return None
        
        progress_bar.progress(50)
        status_text.text(f"æ­£åœ¨é¢„æµ‹ {len(smiles_list)} ä¸ªåˆ†å­...")
        
        # æ­¥éª¤4: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ‰¹é‡é¢„æµ‹
        predictions = trained_model.predict(embeddings)
        
        # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼‰
        probabilities = None
        confidence_scores = None
        if task_type == "åˆ†ç±»" and hasattr(trained_model, 'predict_proba'):
            probabilities = trained_model.predict_proba(embeddings)
            confidence_scores = np.max(probabilities, axis=1)
        
        progress_bar.progress(100)
        status_text.text("é¢„æµ‹å®Œæˆ!")
        
        # è¿”å›é¢„æµ‹ç»“æœ
        return {
            'smiles_list': smiles_list,
            'predictions': predictions,
            'probabilities': probabilities,
            'confidence_scores': confidence_scores,
            'task_type': task_type,
            'algorithm': algorithm,
            'embedding_method': embedding_method
        }
        
    except Exception as e:
        st.error(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def display_batch_prediction_result(result):
    """æ˜¾ç¤ºæ‰¹é‡åˆ†å­é¢„æµ‹ç»“æœ"""
    st.subheader("ğŸ’¡ æ‰¹é‡é¢„æµ‹ç»“æœ")
    
    smiles_list = result['smiles_list']
    predictions = result['predictions']
    
    result_data = {
        'SMILES': smiles_list,
        'é¢„æµ‹æ ‡ç­¾': predictions
    }
    
    if result.get('confidence_scores') is not None:
        result_data['ç½®ä¿¡åº¦'] = result['confidence_scores']
    
    result_df = pd.DataFrame(result_data)
    
    # æ˜¾ç¤ºç»“æœè¡¨æ ¼
    st.dataframe(result_df.style.format({'ç½®ä¿¡åº¦': '{:.2%}'}))
    
    # ç»“æœæ‘˜è¦
    st.subheader("ğŸ“Š ç»“æœæ‘˜è¦")
    if result.get('task_type') == 'å›å½’':
        st.write("**é¢„æµ‹å€¼ç»Ÿè®¡**:")
        st.dataframe(result_df['é¢„æµ‹æ ‡ç­¾'].describe())
    else: # åˆ†ç±»
        st.write("**é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ**:")
        st.dataframe(result_df['é¢„æµ‹æ ‡ç­¾'].value_counts())

def render_project_viewer():
    """æ¸²æŸ“é¡¹ç›®æŸ¥çœ‹é¡µé¢"""
    st.title("ğŸ“ ä¸ªäººé¡¹ç›®ç®¡ç†")
    
    # è·å–ç”¨æˆ·æ•°æ®ç®¡ç†å™¨
    from utils.user_data_utils import UserDataManager
    user_data = UserDataManager(st.session_state.username)
    
    # è·å–ç”¨æˆ·é¡¹ç›®
    projects = user_data.get_user_projects()
    
    # é¡¹ç›®ç»Ÿè®¡
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("åµŒå…¥é¡¹ç›®", len(projects['embeddings']))
    with col2:
        st.metric("æ¨¡å‹é¡¹ç›®", len(projects['models']))
    with col3:
        st.metric("ä¸Šä¼ æ–‡ä»¶", len(projects['uploads']))
    with col4:
        total_size = sum(f['size'] for f in projects['uploads'])
        st.metric("å­˜å‚¨ç©ºé—´", f"{total_size/1024/1024:.1f} MB")
    
    # é¡¹ç›®è¯¦æƒ…å±•ç¤º
    tab1, tab2, tab3 = st.tabs(["ğŸ”¬ åµŒå…¥é¡¹ç›®", "ğŸ¤– æ¨¡å‹é¡¹ç›®", "ğŸ“ ä¸Šä¼ æ–‡ä»¶"])
    
    with tab1:
        st.subheader("åˆ†å­åµŒå…¥é¡¹ç›®")
        if projects['embeddings']:
            for proj in projects['embeddings']:
                with st.expander(f"{proj['project_name']} - {proj['method']}"):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.write(f"**é¡¹ç›®åç§°**: {proj['project_name']}")
                        st.write(f"**åµŒå…¥æ–¹æ³•**: {proj['method']}")
                        st.write(f"**åˆ†å­æ•°é‡**: {proj['num_molecules']}")
                        st.write(f"**åˆ›å»ºæ—¶é—´**: {proj['created_at']}")
                    
                    with col2:
                        if st.button(f"åˆ é™¤", key=f"del_emb_{proj['project_id']}"):
                            if user_data.delete_project('embeddings', proj['project_id']):
                                st.success("é¡¹ç›®å·²åˆ é™¤")
                                st.rerun()
                            else:
                                st.error("åˆ é™¤å¤±è´¥")
        else:
            st.info("æš‚æ— åµŒå…¥é¡¹ç›®")
    
    with tab2:
        st.subheader("æœºå™¨å­¦ä¹ æ¨¡å‹")
        st.info("ğŸ’¡ ç›®å‰å¹³å°é‡‡ç”¨å®æ—¶é¢„æµ‹æ¨¡å¼ï¼Œæ¨¡å‹è®­ç»ƒå®Œæˆåå¯ç«‹å³è¿›è¡Œé¢„æµ‹ï¼Œæ— éœ€ä¿å­˜æ¨¡å‹æ–‡ä»¶")
        st.write("**å®æ—¶é¢„æµ‹çš„ä¼˜åŠ¿**:")
        st.write("- âœ… æ— éœ€æ‹…å¿ƒæ¨¡å‹ä¿å­˜å’ŒåŠ è½½çš„å…¼å®¹æ€§é—®é¢˜")
        st.write("- âœ… è®­ç»ƒå®Œæˆç«‹å³å¯ç”¨ï¼Œæ“ä½œæ›´ç®€å•")
        st.write("- âœ… é¿å…å­˜å‚¨ç©ºé—´å ç”¨")
        st.write("- âœ… å§‹ç»ˆä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹")
        
        st.markdown("---")
        st.write("å¦‚éœ€è¿›è¡Œé¢„æµ‹ï¼Œè¯·å‰å¾€ **ğŸ§  æ™ºèƒ½å»ºæ¨¡** é¡µé¢è®­ç»ƒæ¨¡å‹ï¼Œè®­ç»ƒå®Œæˆåå³å¯ç›´æ¥é¢„æµ‹ã€‚")
    
    with tab3:
        st.subheader("ä¸Šä¼ æ–‡ä»¶")
        if projects['uploads']:
            for file_info in projects['uploads']:
                with st.expander(f"{file_info['filename']}"):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.write(f"**æ–‡ä»¶å¤§å°**: {file_info['size']/1024:.1f} KB")
                        st.write(f"**ä¸Šä¼ æ—¶é—´**: {file_info['created_at']}")
                    
                    with col2:
                        # ä¸‹è½½æŒ‰é’®
                        with open(file_info['filepath'], 'rb') as f:
                            st.download_button(
                                label="ä¸‹è½½",
                                data=f.read(),
                                file_name=file_info['filename'],
                                key=f"download_{file_info['filename']}"
                            )
        else:
            st.info("æš‚æ— ä¸Šä¼ æ–‡ä»¶")
    
    # å¯¼å‡ºåŠŸèƒ½
    st.subheader("ğŸ“¤ æ•°æ®å¯¼å‡º")
    if st.button("å¯¼å‡ºé¡¹ç›®æ‘˜è¦"):
        summary = user_data.export_project_summary()
        
        import json
        json_str = json.dumps(summary, ensure_ascii=False, indent=2)
        
        st.download_button(
            label="ä¸‹è½½é¡¹ç›®æ‘˜è¦ (JSON)",
            data=json_str,
            file_name=f"{st.session_state.username}_projects_summary.json",
            mime="application/json"
        )

def render_literature_mining():
    """æ¸²æŸ“æ–‡çŒ®æŒ–æ˜é¡µé¢"""
    st.markdown('<h1 class="main-title">ğŸ“š æ–‡çŒ®æŒ–æ˜</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #7f8c8d; margin-bottom: 2rem;">åŸºäºæ–‡çŒ®æ•°æ®åº“çš„æ™ºèƒ½æ£€ç´¢ä¸åˆ†æç³»ç»Ÿ</p>', unsafe_allow_html=True)
    
    # å¯¼å…¥æ–‡çŒ®æŒ–æ˜å·¥å…·
    from utils.literature_utils import LiteratureMiner, export_to_bibtex
    import json
    import requests
    
    # åˆå§‹åŒ–æ–‡çŒ®æŒ–æ˜å™¨
    if 'literature_miner' not in st.session_state:
        st.session_state.literature_miner = LiteratureMiner()
    
    # æœç´¢ç­–ç•¥é€‰æ‹©
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        search_mode = st.selectbox(
            "ğŸ” é€‰æ‹©æœç´¢æ¨¡å¼",
            ["ç»¼åˆæœç´¢ (æ¨è)", "PubMedä¸“é¡¹", "CrossRefä¸“é¡¹", "Semantic Scholarä¸“é¡¹"],
            help="ç»¼åˆæœç´¢ä¼šåŒæ—¶ä½¿ç”¨å¤šä¸ªæ•°æ®åº“å¹¶å»é‡ï¼Œè·å¾—æ›´å…¨é¢çš„ç»“æœ"
        )
    
    with col2:
        max_results = st.slider("æ¯ä¸ªæ•°æ®åº“æœ€å¤§ç»“æœæ•°", 5, 50, 15)
    
    with col3:
        enable_suggestions = st.checkbox("å¯ç”¨æœç´¢å»ºè®®", value=True)
    
    # æœç´¢è¾“å…¥
    query = st.text_input(
        "ğŸ” è¾“å…¥æœç´¢å…³é”®è¯", 
        placeholder="ä¾‹å¦‚: QSAR machine learning drug discovery",
        help="æ”¯æŒè‹±æ–‡å…³é”®è¯æœç´¢ï¼Œå¯ä»¥ä½¿ç”¨å¸ƒå°”æ“ä½œç¬¦ AND, OR, NOT"
    )
    
    # æœç´¢å»ºè®®
    if enable_suggestions and query and len(query) > 3:
        suggestions = st.session_state.literature_miner.generate_search_suggestions(query)
        if suggestions:
            st.markdown("ğŸ’¡ **æœç´¢å»ºè®®**:")
            suggestion_cols = st.columns(len(suggestions))
            for i, suggestion in enumerate(suggestions):
                with suggestion_cols[i]:
                    if st.button(f"ğŸ” {suggestion[:30]}...", key=f"suggestion_{i}"):
                        query = suggestion
                        st.rerun()
    
    # é«˜çº§æœç´¢é€‰é¡¹
    with st.expander("ğŸ”§ é«˜çº§æœç´¢é€‰é¡¹"):
        col1, col2 = st.columns(2)
        with col1:
            year_filter = st.text_input(
                "å¹´ä»½ç­›é€‰", 
                placeholder="ä¾‹å¦‚: 2020:2024[dp] (PubMedæ ¼å¼)",
                help="PubMedæ”¯æŒæ—¥æœŸèŒƒå›´æ ¼å¼ï¼Œå¦‚ 2020:2024[dp]"
            )
        with col2:
            additional_terms = st.text_input(
                "é™„åŠ æœç´¢è¯", 
                placeholder="ä¾‹å¦‚: review, meta-analysis",
                help="æ·»åŠ é¢å¤–çš„æœç´¢é™å®šè¯"
            )
    
    # æ„å»ºæœ€ç»ˆæŸ¥è¯¢
    final_query = query
    if year_filter:
        final_query += f" AND {year_filter}"
    if additional_terms:
        final_query += f" AND {additional_terms}"
    
    # æœç´¢æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æœç´¢", type="primary"):
        if query:
            try:
                # æ ¹æ®æœç´¢æ¨¡å¼æ‰§è¡Œæœç´¢
                if search_mode == "ç»¼åˆæœç´¢ (æ¨è)":
                    results = st.session_state.literature_miner.comprehensive_search(final_query, max_results)
                elif search_mode == "PubMedä¸“é¡¹":
                    with st.spinner("æ­£åœ¨æœç´¢PubMedæ•°æ®åº“..."):
                        results = st.session_state.literature_miner.search_pubmed(final_query, max_results * 3)
                elif search_mode == "CrossRefä¸“é¡¹":
                    with st.spinner("æ­£åœ¨æœç´¢CrossRefæ•°æ®åº“..."):
                        results = st.session_state.literature_miner.search_crossref(final_query, max_results * 3)
                else:  # Semantic Scholarä¸“é¡¹
                    with st.spinner("æ­£åœ¨æœç´¢Semantic Scholaræ•°æ®åº“..."):
                        results = st.session_state.literature_miner.search_semantic_scholar(final_query, max_results * 3)
                
                if results:
                    st.success(f"ğŸ‰ æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³æ–‡çŒ®")
                    
                    # æ•°æ®æºç»Ÿè®¡
                    source_counts = pd.Series([r['source'] for r in results]).value_counts()
                    st.markdown("### ğŸ“Š æ•°æ®æºåˆ†å¸ƒ")
                    source_cols = st.columns(len(source_counts))
                    for i, (source, count) in enumerate(source_counts.items()):
                        with source_cols[i]:
                            st.metric(source, count)
                    
                    # æœç´¢ç»“æœç»Ÿè®¡
                    st.markdown("### ğŸ“ˆ æœç´¢ç»“æœåˆ†æ")
                    
                    # åˆ›å»ºç»“æœDataFrame
                    df_results = pd.DataFrame(results)
                    
                    # ç»Ÿè®¡å›¾è¡¨
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        # å¹´ä»½åˆ†å¸ƒ
                        year_counts = df_results['year'].value_counts().sort_index()
                        fig_year = plt.figure(figsize=(8, 4))
                        plt.bar(year_counts.index, year_counts.values, color='skyblue', alpha=0.7)
                        plt.title('Publication Year Distribution')
                        plt.xlabel('Year')
                        plt.ylabel('Number of Papers')
                        plt.xticks(rotation=45)
                        plt.tight_layout()
                        st.pyplot(fig_year)
                        plt.close()
                    
                    with col2:
                        # æœŸåˆŠåˆ†å¸ƒ
                        journal_counts = df_results['journal'].value_counts().head(10)
                        fig_journal = plt.figure(figsize=(8, 6))
                        plt.barh(range(len(journal_counts)), journal_counts.values, color='lightcoral', alpha=0.7)
                        plt.yticks(range(len(journal_counts)), journal_counts.index)
                        plt.title('Top Journals Distribution')
                        plt.xlabel('Number of Papers')
                        plt.tight_layout()
                        st.pyplot(fig_journal)
                        plt.close()
                    
                    with col3:
                        # å¼•ç”¨æ•°åˆ†å¸ƒ
                        citation_ranges = ['0-10', '11-50', '51-100', '100+']
                        citation_counts = [
                            len([r for r in results if 0 <= r['citations'] <= 10]),
                            len([r for r in results if 11 <= r['citations'] <= 50]),
                            len([r for r in results if 51 <= r['citations'] <= 100]),
                            len([r for r in results if r['citations'] > 100])
                        ]
                        
                        fig_citations = plt.figure(figsize=(8, 4))
                        plt.bar(citation_ranges, citation_counts, color='lightgreen', alpha=0.7)
                        plt.title('Citation Count Distribution')
                        plt.xlabel('Citation Range')
                        plt.ylabel('Number of Papers')
                        plt.tight_layout()
                        st.pyplot(fig_citations)
                        plt.close()
                    
                    # è¯¦ç»†ç»“æœå±•ç¤º
                    st.markdown("### ğŸ“„ è¯¦ç»†æœç´¢ç»“æœ")
                    
                    # æ’åºé€‰é¡¹
                    sort_option = st.selectbox(
                        "æ’åºæ–¹å¼", 
                        ["æŒ‰å¼•ç”¨æ•°é™åº", "æŒ‰å¹´ä»½é™åº", "æŒ‰å½±å“å› å­é™åº", "æŒ‰ç›¸å…³æ€§"],
                        key="sort_results"
                    )
                    
                    if sort_option == "æŒ‰å¼•ç”¨æ•°é™åº":
                        results.sort(key=lambda x: x['citations'], reverse=True)
                    elif sort_option == "æŒ‰å¹´ä»½é™åº":
                        results.sort(key=lambda x: x['year'], reverse=True)
                    elif sort_option == "æŒ‰å½±å“å› å­é™åº":
                        results.sort(key=lambda x: x['impact_factor'], reverse=True)
                    
                    for i, result in enumerate(results, 1):
                        with st.expander(f"ğŸ“‘ {i}. {result['title'][:80]}... [{result['source']}]"):
                            col1, col2 = st.columns([3, 1])
                            
                            with col1:
                                st.markdown(f"**æ ‡é¢˜**: {result['title']}")
                                st.markdown(f"**ä½œè€…**: {', '.join(result['authors'][:5])}{'...' if len(result['authors']) > 5 else ''}")
                                st.markdown(f"**æœŸåˆŠ**: {result['journal']}")
                                st.markdown(f"**å¹´ä»½**: {result['year']}")
                                if result['abstract']:
                                    st.markdown(f"**æ‘˜è¦**: {result['abstract'][:400]}...")
                                if result['doi']:
                                    st.markdown(f"**DOI**: [{result['doi']}](https://doi.org/{result['doi']})")
                                if result['pmid']:
                                    st.markdown(f"**PMID**: [{result['pmid']}](https://pubmed.ncbi.nlm.nih.gov/{result['pmid']}/)")
                            
                            with col2:
                                st.metric("å¼•ç”¨æ•°", result['citations'])
                                st.metric("å½±å“å› å­", f"{result['impact_factor']:.2f}")
                                st.markdown(f"**æ•°æ®æº**: {result['source']}")
                    
                    # å¯¼å‡ºé€‰é¡¹
                    st.markdown("### ğŸ’¾ å¯¼å‡ºç»“æœ")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        # CSVå¯¼å‡º
                        csv_data = df_results.to_csv(index=False)
                        st.download_button(
                            label="ğŸ“Š ä¸‹è½½CSVæ ¼å¼",
                            data=csv_data,
                            file_name=f"literature_search_{query.replace(' ', '_')[:20]}.csv",
                            mime="text/csv"
                        )
                    
                    with col2:
                        # BibTeXå¯¼å‡º
                        bibtex_data = export_to_bibtex(results)
                        st.download_button(
                            label="ğŸ“š ä¸‹è½½BibTeXæ ¼å¼",
                            data=bibtex_data,
                            file_name=f"literature_search_{query.replace(' ', '_')[:20]}.bib",
                            mime="text/plain"
                        )
                    
                    with col3:
                        # JSONå¯¼å‡º
                        json_data = json.dumps(results, indent=2, ensure_ascii=False)
                        st.download_button(
                            label="ğŸ“‹ ä¸‹è½½JSONæ ¼å¼",
                            data=json_data,
                            file_name=f"literature_search_{query.replace(' ', '_')[:20]}.json",
                            mime="application/json"
                        )
                    

                    # ä¿å­˜åˆ°ä¸ªäººé¡¹ç›®
                    if st.button("ğŸ’¾ ä¿å­˜åˆ°ä¸ªäººé¡¹ç›®"):
                        from utils.user_data_utils import UserDataManager
                        user_data = UserDataManager(st.session_state.username)
                        
                        project_data = {
                            'query': final_query,
                            'search_mode': search_mode,
                            'results_count': len(results),
                            'results': results,

                            'timestamp': pd.Timestamp.now().isoformat()
                        }
                        
                        filename = f"literature_search_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
                        filepath = user_data.save_project_data(filename, project_data, 'literature')
                        st.success(f"âœ… æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
                
                else:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š")
                    st.markdown("""
                    - ä½¿ç”¨æ›´é€šç”¨çš„å…³é”®è¯
                    - æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®
                    - å°è¯•è‹±æ–‡å…³é”®è¯
                    - ä½¿ç”¨åŒä¹‰è¯æˆ–ç›¸å…³æœ¯è¯­
                    - å‡å°‘æœç´¢è¯çš„æ•°é‡
                    """)
                    
            except Exception as e:
                st.error(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                st.markdown("""
                **å¯èƒ½çš„åŸå› ï¼š**
                - ç½‘ç»œè¿æ¥é—®é¢˜
                - APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨
                - æœç´¢æŸ¥è¯¢æ ¼å¼ä¸æ­£ç¡®
                
                **å»ºè®®ï¼š**
                - æ£€æŸ¥ç½‘ç»œè¿æ¥
                - ç¨åé‡è¯•
                - ç®€åŒ–æœç´¢å…³é”®è¯
                """)
        else:
            st.error("âŒ è¯·è¾“å…¥æœç´¢å†…å®¹")
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### ğŸ” æœç´¢æŠ€å·§
        
        **å…³é”®è¯é€‰æ‹©ï¼š**
        - ä½¿ç”¨è‹±æ–‡å…³é”®è¯æ•ˆæœæ›´ä½³
        - å¯ä»¥ä½¿ç”¨å¸ƒå°”æ“ä½œç¬¦ï¼šAND, OR, NOT
        - ä½¿ç”¨å¼•å·åŒ…å›´çŸ­è¯­ï¼š"machine learning"
        
        **æœç´¢æ¨¡å¼ï¼š**
        - **ç»¼åˆæœç´¢**ï¼šåŒæ—¶æœç´¢å¤šä¸ªæ•°æ®åº“ï¼Œç»“æœæ›´å…¨é¢
        - **PubMedä¸“é¡¹**ï¼šä¸“æ³¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®
        - **CrossRefä¸“é¡¹**ï¼šè¦†ç›–æ›´å¹¿æ³›çš„å­¦ç§‘é¢†åŸŸ
        - **Semantic Scholarä¸“é¡¹**ï¼šAIå¢å¼ºçš„å­¦æœ¯æœç´¢
        
        **é«˜çº§åŠŸèƒ½ï¼š**
        - å¹´ä»½ç­›é€‰ï¼šä½¿ç”¨PubMedæ ¼å¼ "2020:2024[dp]"
        - æœç´¢å»ºè®®ï¼šç³»ç»Ÿä¼šæ ¹æ®è¾“å…¥æä¾›ç›¸å…³å»ºè®®
        - å¤šç§å¯¼å‡ºæ ¼å¼ï¼šCSVã€BibTeXã€JSON
        
        ### ğŸ“Š æ•°æ®æ¥æº
        - **PubMed**: ç¾å›½å›½ç«‹åŒ»å­¦å›¾ä¹¦é¦†ç”Ÿç‰©åŒ»å­¦æ•°æ®åº“
        - **CrossRef**: å­¦æœ¯å‡ºç‰ˆç‰©DOIæ³¨å†Œæœºæ„
        - **Semantic Scholar**: AIé©±åŠ¨çš„å­¦æœ¯æœç´¢å¼•æ“
        
        ### ğŸ¤– AIæ‘˜è¦åŠŸèƒ½
        - **ç»¼åˆæ‘˜è¦**: å…¨é¢åˆ†ææ–‡çŒ®å†…å®¹ï¼Œæ€»ç»“ç ”ç©¶æ–¹å‘å’Œå‘ç°
        - **æŠ€æœ¯æ–¹æ³•æ€»ç»“**: é‡ç‚¹åˆ†ææŠ€æœ¯æ–¹æ³•å’Œç®—æ³•
        - **ç ”ç©¶è¶‹åŠ¿åˆ†æ**: è¯†åˆ«ç ”ç©¶çƒ­ç‚¹å’Œå‘å±•è¶‹åŠ¿
        - **å…³é”®å‘ç°æå–**: æå–æ ¸å¿ƒå‘ç°å’Œé‡è¦ç»“è®º
        - **æ‰¹é‡æ‘˜è¦**: å¯¹å•ç¯‡æ–‡çŒ®è¿›è¡Œå¿«é€Ÿæ‘˜è¦
        - **ç ”ç©¶é—®é¢˜ç”Ÿæˆ**: åŸºäºæ–‡çŒ®å†…å®¹ç”Ÿæˆæœ‰ä»·å€¼çš„ç ”ç©¶é—®é¢˜
        
        **æ”¯æŒæ¨¡å‹**: Qwen/QwQ-32B, Qwen2.5ç³»åˆ—ç­‰ç¡…åŸºæµåŠ¨å¹³å°æ¨¡å‹
        """)
    
    # APIçŠ¶æ€æ£€æŸ¥
    with st.expander("ğŸ”§ APIçŠ¶æ€æ£€æŸ¥"):
        if st.button("æ£€æŸ¥APIè¿æ¥çŠ¶æ€"):
            status_col1, status_col2, status_col3 = st.columns(3)
            
            with status_col1:
                try:
                    response = requests.get("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi", timeout=5)
                    if response.status_code == 200:
                        st.success("âœ… PubMed API æ­£å¸¸")
                    else:
                        st.error("âŒ PubMed API å¼‚å¸¸")
                except:
                    st.error("âŒ PubMed API è¿æ¥å¤±è´¥")
            
            with status_col2:
                try:
                    response = requests.get("https://api.crossref.org/works?rows=1", timeout=5)
                    if response.status_code == 200:
                        st.success("âœ… CrossRef API æ­£å¸¸")
                    else:
                        st.error("âŒ CrossRef API å¼‚å¸¸")
                except:
                    st.error("âŒ CrossRef API è¿æ¥å¤±è´¥")
            
            with status_col3:
                try:
                    response = requests.get("https://api.semanticscholar.org/graph/v1/paper/search?query=test&limit=1", timeout=5)
                    if response.status_code == 200:
                        st.success("âœ… Semantic Scholar API æ­£å¸¸")
                    else:
                        st.error("âŒ Semantic Scholar API å¼‚å¸¸")
                except:
                    st.error("âŒ Semantic Scholar API è¿æ¥å¤±è´¥")

def render_ai_summary():
    """æ¸²æŸ“AIæ‘˜è¦æ€»ç»“é¡µé¢"""
    st.markdown('<h1 class="main-title">ğŸ¤– AIæ–‡çŒ®æ‘˜è¦æ€»ç»“</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #7f8c8d; margin-bottom: 2rem;">æ™ºèƒ½æç‚¼æ–‡çŒ®æ ¸å¿ƒè¦ç‚¹ï¼Œä¸­æ–‡è¾“å‡ºå…³é”®ä¿¡æ¯</p>', unsafe_allow_html=True)
    
    # å¯¼å…¥AIæ‘˜è¦å·¥å…·
    from utils.ai_summary_utils import AISummaryGenerator, test_api_connection
    import json
    
    # åˆå§‹åŒ–AIæ‘˜è¦ç”Ÿæˆå™¨
    if 'ai_summary_generator' not in st.session_state:
        st.session_state.ai_summary_generator = AISummaryGenerator()
    
    # APIé…ç½®åŒºåŸŸ
    st.markdown("### âš™ï¸ APIé…ç½®")
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        api_token = st.text_input(
            "ğŸ”‘ ç¡…åŸºæµåŠ¨API Token", 
            type="password",
            placeholder="è¯·è¾“å…¥æ‚¨çš„API Token",
            help="è·å–Token: https://cloud.siliconflow.cn/"
        )
    
    with col2:
        model_input_method = st.radio(
            "æ¨¡å‹é€‰æ‹©æ–¹å¼",
            ["é¢„è®¾æ¨¡å‹", "è‡ªå®šä¹‰æ¨¡å‹"],
            horizontal=True,
            help="é€‰æ‹©ä½¿ç”¨é¢„è®¾æ¨¡å‹è¿˜æ˜¯è‡ªå®šä¹‰è¾“å…¥æ¨¡å‹åç§°"
        )
    
    with col3:
        if model_input_method == "é¢„è®¾æ¨¡å‹":
            selected_model = st.selectbox(
                "ğŸ§  é€‰æ‹©AIæ¨¡å‹",
                st.session_state.ai_summary_generator.available_models,
                index=0,
                help="ä¸åŒæ¨¡å‹æ•ˆæœå’Œé€Ÿåº¦æœ‰å·®å¼‚"
            )
        else:
            selected_model = st.text_input(
                "ğŸ§  è‡ªå®šä¹‰æ¨¡å‹åç§°",
                placeholder="ä¾‹å¦‚ï¼šQwen/Qwen2.5-7B-Instruct",
                help="è¾“å…¥å®Œæ•´çš„æ¨¡å‹åç§°ï¼Œå¦‚ Qwen/Qwen2.5-7B-Instruct"
            )
    
    # APIè¿æ¥æµ‹è¯•
    if api_token and selected_model:
        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("ğŸ”§ æµ‹è¯•APIè¿æ¥"):
                with st.spinner("æµ‹è¯•è¿æ¥ä¸­..."):
                    if test_api_connection(api_token, selected_model):
                        st.success("âœ… APIè¿æ¥æ­£å¸¸")
                    else:
                        st.error("âŒ APIè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥Tokenå’Œæ¨¡å‹åç§°")
        with col2:
            if selected_model:
                st.info(f"å½“å‰æ¨¡å‹: {selected_model}")
    elif api_token and not selected_model:
        st.warning("âš ï¸ è¯·é€‰æ‹©æˆ–è¾“å…¥æ¨¡å‹åç§°")
    elif not api_token and selected_model:
        st.warning("âš ï¸ è¯·è¾“å…¥API Token")
    
    st.markdown("---")
    
    # æ‘˜è¦è¾“å…¥åŒºåŸŸ
    st.markdown("### ğŸ“ æ–‡çŒ®æ‘˜è¦è¾“å…¥")
    
    # è¾“å…¥æ–¹å¼é€‰æ‹©
    input_method = st.radio(
        "é€‰æ‹©è¾“å…¥æ–¹å¼",
        ["ğŸ“„ å•ç¯‡æ‘˜è¦", "ğŸ“š æ‰¹é‡æ‘˜è¦", "ğŸ“‹ ä»å‰ªè´´æ¿ç²˜è´´"],
        horizontal=True
    )
    
    abstracts_to_process = []
    
    if input_method == "ğŸ“„ å•ç¯‡æ‘˜è¦":
        st.markdown("#### è¾“å…¥å•ç¯‡æ–‡çŒ®æ‘˜è¦")
        
        col1, col2 = st.columns([1, 1])
        with col1:
            title = st.text_input("æ–‡çŒ®æ ‡é¢˜ (å¯é€‰)", placeholder="ä¾‹å¦‚ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„QSARæ¨¡å‹ç ”ç©¶")
        with col2:
            authors = st.text_input("ä½œè€… (å¯é€‰)", placeholder="ä¾‹å¦‚ï¼šå¼ ä¸‰, æå››")
        
        abstract_text = st.text_area(
            "æ–‡çŒ®æ‘˜è¦",
            height=200,
            placeholder="è¯·ç²˜è´´æ–‡çŒ®æ‘˜è¦å†…å®¹...",
            help="æ”¯æŒä¸­è‹±æ–‡æ‘˜è¦ï¼Œå»ºè®®å­—æ•°åœ¨100-2000å­—ä¹‹é—´"
        )
        
        if abstract_text.strip():
            abstracts_to_process.append({
                'title': title or "æœªæä¾›æ ‡é¢˜",
                'authors': [authors] if authors else [],
                'abstract': abstract_text.strip(),
                'year': 2024,
                'journal': "ç”¨æˆ·è¾“å…¥",
                'citations': 0
            })
    
    elif input_method == "ğŸ“š æ‰¹é‡æ‘˜è¦":
        st.markdown("#### æ‰¹é‡è¾“å…¥å¤šç¯‡æ‘˜è¦")
        st.info("ğŸ’¡ æ¯è¡Œä¸€ç¯‡æ‘˜è¦ï¼Œæˆ–ç”¨ç©ºè¡Œåˆ†éš”å¤šç¯‡æ‘˜è¦")
        
        batch_text = st.text_area(
            "æ‰¹é‡æ‘˜è¦è¾“å…¥",
            height=300,
            placeholder="æ‘˜è¦1ï¼šè¿™æ˜¯ç¬¬ä¸€ç¯‡æ–‡çŒ®çš„æ‘˜è¦å†…å®¹...\n\næ‘˜è¦2ï¼šè¿™æ˜¯ç¬¬äºŒç¯‡æ–‡çŒ®çš„æ‘˜è¦å†…å®¹...\n\næ‘˜è¦3ï¼šè¿™æ˜¯ç¬¬ä¸‰ç¯‡æ–‡çŒ®çš„æ‘˜è¦å†…å®¹...",
            help="æ”¯æŒå¤šç§åˆ†éš”æ–¹å¼ï¼šç©ºè¡Œã€åºå·ã€æˆ–æ¯è¡Œä¸€ç¯‡"
        )
        
        if batch_text.strip():
            # è§£ææ‰¹é‡è¾“å…¥
            abstracts = []
            
            # æŒ‰ç©ºè¡Œåˆ†å‰²
            parts = [part.strip() for part in batch_text.split('\n\n') if part.strip()]
            
            if len(parts) == 1:
                # å¦‚æœæ²¡æœ‰ç©ºè¡Œåˆ†å‰²ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
                lines = [line.strip() for line in batch_text.split('\n') if line.strip()]
                if len(lines) > 1:
                    parts = lines
            
            for i, part in enumerate(parts, 1):
                # ç§»é™¤å¯èƒ½çš„åºå·
                clean_text = part
                if part.startswith(f"{i}.") or part.startswith(f"{i}ã€") or part.startswith(f"æ‘˜è¦{i}"):
                    clean_text = part.split('ï¼š', 1)[-1].split(':', 1)[-1].strip()
                
                if len(clean_text) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                    abstracts_to_process.append({
                        'title': f"æ‰¹é‡è¾“å…¥æ–‡çŒ® {i}",
                        'authors': [],
                        'abstract': clean_text,
                        'year': 2024,
                        'journal': "ç”¨æˆ·è¾“å…¥",
                        'citations': 0
                    })
    
    else:  # ä»å‰ªè´´æ¿ç²˜è´´
        st.markdown("#### ä»å‰ªè´´æ¿ç²˜è´´")
        st.info("ğŸ’¡ ç›´æ¥ç²˜è´´ä»å…¶ä»–åœ°æ–¹å¤åˆ¶çš„æ‘˜è¦å†…å®¹")
        
        clipboard_text = st.text_area(
            "ç²˜è´´æ‘˜è¦å†…å®¹",
            height=250,
            placeholder="Ctrl+V ç²˜è´´æ‘˜è¦å†…å®¹...",
            help="æ”¯æŒä»PDFã€ç½‘é¡µã€æ–‡æ¡£ç­‰å¤åˆ¶çš„å†…å®¹"
        )
        
        if clipboard_text.strip():
            abstracts_to_process.append({
                'title': "å‰ªè´´æ¿è¾“å…¥",
                'authors': [],
                'abstract': clipboard_text.strip(),
                'year': 2024,
                'journal': "ç”¨æˆ·è¾“å…¥",
                'citations': 0
            })
    
    # æ˜¾ç¤ºå¾…å¤„ç†æ‘˜è¦æ•°é‡
    if abstracts_to_process:
        st.success(f"âœ… å·²å‡†å¤‡ {len(abstracts_to_process)} ç¯‡æ‘˜è¦å¾…å¤„ç†")
        
        # é¢„è§ˆæ‘˜è¦
        with st.expander("ğŸ‘€ é¢„è§ˆå¾…å¤„ç†æ‘˜è¦"):
            for i, abstract in enumerate(abstracts_to_process, 1):
                st.markdown(f"**{i}. {abstract['title']}**")
                st.markdown(f"æ‘˜è¦: {abstract['abstract'][:200]}...")
                st.markdown("---")
    
    st.markdown("---")
    
    # AIå¤„ç†åŒºåŸŸ
    if abstracts_to_process and api_token and selected_model:
        st.markdown("### ğŸš€ AIæ™ºèƒ½åˆ†æ")
        
        # åˆ†æé€‰é¡¹
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ¯ æç‚¼æ ¸å¿ƒè¦ç‚¹", type="primary"):
                try:
                    with st.spinner("AIæ­£åœ¨åˆ†ææ‘˜è¦ï¼Œæç‚¼æ ¸å¿ƒè¦ç‚¹..."):
                        # ä¸ºå•ä¸ªæ‘˜è¦åˆ†ææ„å»ºç‰¹æ®Šæç¤ºè¯
                        if len(abstracts_to_process) == 1:
                            abstract = abstracts_to_process[0]
                            prompt = f"""
è¯·å¯¹ä»¥ä¸‹æ–‡çŒ®æ‘˜è¦è¿›è¡Œæ™ºèƒ½åˆ†æï¼Œæç‚¼æ ¸å¿ƒè¦ç‚¹ï¼Œç”¨ä¸­æ–‡è¾“å‡ºï¼š

æ ‡é¢˜ï¼š{abstract['title']}
æ‘˜è¦ï¼š{abstract['abstract']}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

**ğŸ¯ æ ¸å¿ƒç ”ç©¶å†…å®¹**
- [æç‚¼ç ”ç©¶çš„æ ¸å¿ƒå†…å®¹å’Œç›®æ ‡]

**ğŸ”¬ ä¸»è¦æ–¹æ³•**
- [æ€»ç»“ä½¿ç”¨çš„ä¸»è¦ç ”ç©¶æ–¹æ³•]

**ğŸ“Š å…³é”®å‘ç°**
- [åˆ—å‡ºé‡è¦çš„ç ”ç©¶å‘ç°å’Œç»“æœ]

**ğŸ’¡ åˆ›æ–°ç‚¹**
- [æŒ‡å‡ºç ”ç©¶çš„åˆ›æ–°ä¹‹å¤„]

**ğŸ” ç ”ç©¶æ„ä¹‰**
- [è¯´æ˜ç ”ç©¶çš„ç†è®ºå’Œå®é™…æ„ä¹‰]

è¦æ±‚ï¼š
1. ç”¨ç®€æ´æ˜äº†çš„ä¸­æ–‡è¡¨è¾¾
2. çªå‡ºæœ€é‡è¦çš„ä¿¡æ¯
3. æ¯ä¸ªè¦ç‚¹æ§åˆ¶åœ¨1-2å¥è¯
4. é¿å…é‡å¤åŸæ–‡è¡¨è¿°
"""
                        else:
                            # å¤šä¸ªæ‘˜è¦çš„ç»¼åˆåˆ†æ
                            abstracts_text = "\n\n".join([
                                f"æ–‡çŒ®{i+1}ï¼š{abs['title']}\næ‘˜è¦ï¼š{abs['abstract']}"
                                for i, abs in enumerate(abstracts_to_process)
                            ])
                            
                            prompt = f"""
è¯·å¯¹ä»¥ä¸‹å¤šç¯‡æ–‡çŒ®æ‘˜è¦è¿›è¡Œç»¼åˆåˆ†æï¼Œæç‚¼å…±åŒçš„æ ¸å¿ƒè¦ç‚¹ï¼Œç”¨ä¸­æ–‡è¾“å‡ºï¼š

{abstracts_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

**ğŸ¯ å…±åŒç ”ç©¶ä¸»é¢˜**
- [æ€»ç»“è¿™äº›æ–‡çŒ®çš„å…±åŒç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜]

**ğŸ”¬ ä¸»è¦ç ”ç©¶æ–¹æ³•**
- [å½’çº³ä½¿ç”¨çš„ä¸»è¦ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯]

**ğŸ“Š é‡è¦å‘ç°æ±‡æ€»**
- [æ±‡æ€»å„æ–‡çŒ®çš„é‡è¦å‘ç°]

**ğŸ’¡ æŠ€æœ¯åˆ›æ–°ç‚¹**
- [æ€»ç»“æŠ€æœ¯å’Œæ–¹æ³•ä¸Šçš„åˆ›æ–°]

**ğŸ” ç ”ç©¶è¶‹åŠ¿**
- [åˆ†æä½“ç°çš„ç ”ç©¶è¶‹åŠ¿å’Œæ–¹å‘]

**ğŸš€ æœªæ¥å±•æœ›**
- [åŸºäºè¿™äº›ç ”ç©¶æå‡ºæœªæ¥å‘å±•æ–¹å‘]

è¦æ±‚ï¼š
1. ç”¨ç®€æ´æ˜äº†çš„ä¸­æ–‡è¡¨è¾¾
2. çªå‡ºå…±æ€§å’Œé‡è¦ä¿¡æ¯
3. æ¯ä¸ªè¦ç‚¹æ§åˆ¶åœ¨1-2å¥è¯
4. ä½“ç°ç»¼åˆåˆ†æçš„ä»·å€¼
"""
                        
                        # è°ƒç”¨AI API
                        headers = {
                            'Authorization': f'Bearer {api_token}',
                            'Content-Type': 'application/json'
                        }
                        
                        data = {
                            "model": selected_model,
                            "messages": [{"role": "user", "content": prompt}],
                            "stream": False,
                            "max_tokens": 1500,
                            "temperature": 0.7,
                            "top_p": 0.9
                        }
                        
                        response = requests.post(
                            "https://api.siliconflow.cn/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            if 'choices' in result and len(result['choices']) > 0:
                                summary = result['choices'][0]['message']['content']
                                
                                st.markdown("### ğŸ“‹ AIåˆ†æç»“æœ")
                                st.markdown(f"**ä½¿ç”¨æ¨¡å‹**: {selected_model}")
                                st.markdown(f"**åˆ†ææ–‡çŒ®æ•°**: {len(abstracts_to_process)}")
                                st.markdown("---")
                                st.markdown(summary)
                                
                                # ä¿å­˜ç»“æœ
                                if 'ai_summaries' not in st.session_state:
                                    st.session_state.ai_summaries = []
                                
                                st.session_state.ai_summaries.append({
                                    'summary': summary,
                                    'model': selected_model,
                                    'type': 'æ ¸å¿ƒè¦ç‚¹æç‚¼',
                                    'input_count': len(abstracts_to_process),
                                    'timestamp': pd.Timestamp.now().isoformat()
                                })
                            else:
                                st.error("AIå“åº”æ ¼å¼å¼‚å¸¸")
                        else:
                            st.error(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                            
                except Exception as e:
                    st.error(f"åˆ†æå¤±è´¥: {str(e)}")
        
        with col2:
            if st.button("ğŸ“ˆ ç ”ç©¶æ–¹æ³•åˆ†æ"):
                try:
                    with st.spinner("AIæ­£åœ¨åˆ†æç ”ç©¶æ–¹æ³•..."):
                        abstracts_text = "\n\n".join([
                            f"æ–‡çŒ®{i+1}ï¼š{abs['abstract']}"
                            for i, abs in enumerate(abstracts_to_process)
                        ])
                        
                        prompt = f"""
è¯·é‡ç‚¹åˆ†æä»¥ä¸‹æ–‡çŒ®æ‘˜è¦ä¸­çš„ç ”ç©¶æ–¹æ³•ï¼Œç”¨ä¸­æ–‡è¾“å‡ºï¼š

{abstracts_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

**ğŸ”¬ ä¸»è¦ç ”ç©¶æ–¹æ³•**
- [åˆ—å‡ºä½¿ç”¨çš„ä¸»è¦ç ”ç©¶æ–¹æ³•]

**ğŸ“Š æ•°æ®åˆ†ææŠ€æœ¯**
- [æ€»ç»“æ•°æ®å¤„ç†å’Œåˆ†ææŠ€æœ¯]

**ğŸ› ï¸ å®éªŒè®¾è®¡**
- [æè¿°å®éªŒè®¾è®¡æ€è·¯]

**ğŸ“ è¯„ä¼°æŒ‡æ ‡**
- [åˆ—å‡ºä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’Œæ ‡å‡†]

**âš¡ æ–¹æ³•ä¼˜åŠ¿**
- [åˆ†ææ–¹æ³•çš„ä¼˜åŠ¿å’Œç‰¹ç‚¹]

**ğŸ”„ æ”¹è¿›ç©ºé—´**
- [æŒ‡å‡ºå¯èƒ½çš„æ”¹è¿›æ–¹å‘]

è¦æ±‚ï¼šç®€æ´æ˜äº†ï¼Œçªå‡ºæ–¹æ³•ç‰¹è‰²ï¼Œæ¯ç‚¹1-2å¥è¯ã€‚
"""
                        
                        headers = {
                            'Authorization': f'Bearer {api_token}',
                            'Content-Type': 'application/json'
                        }
                        
                        data = {
                            "model": selected_model,
                            "messages": [{"role": "user", "content": prompt}],
                            "stream": False,
                            "max_tokens": 1200,
                            "temperature": 0.7
                        }
                        
                        response = requests.post(
                            "https://api.siliconflow.cn/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            if 'choices' in result and len(result['choices']) > 0:
                                analysis = result['choices'][0]['message']['content']
                                
                                st.markdown("### ğŸ”¬ ç ”ç©¶æ–¹æ³•åˆ†æ")
                                st.markdown(analysis)
                                
                                # ä¿å­˜ç»“æœ
                                if 'ai_summaries' not in st.session_state:
                                    st.session_state.ai_summaries = []
                                
                                st.session_state.ai_summaries.append({
                                    'summary': analysis,
                                    'model': selected_model,
                                    'type': 'ç ”ç©¶æ–¹æ³•åˆ†æ',
                                    'input_count': len(abstracts_to_process),
                                    'timestamp': pd.Timestamp.now().isoformat()
                                })
                        
                except Exception as e:
                    st.error(f"æ–¹æ³•åˆ†æå¤±è´¥: {str(e)}")
        
        with col3:
            if st.button("ğŸ’¡ åˆ›æ–°ç‚¹æŒ–æ˜"):
                try:
                    with st.spinner("AIæ­£åœ¨æŒ–æ˜åˆ›æ–°ç‚¹..."):
                        abstracts_text = "\n\n".join([
                            f"æ–‡çŒ®{i+1}ï¼š{abs['abstract']}"
                            for i, abs in enumerate(abstracts_to_process)
                        ])
                        
                        prompt = f"""
è¯·æ·±åº¦æŒ–æ˜ä»¥ä¸‹æ–‡çŒ®æ‘˜è¦ä¸­çš„åˆ›æ–°ç‚¹å’Œäº®ç‚¹ï¼Œç”¨ä¸­æ–‡è¾“å‡ºï¼š

{abstracts_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

**ğŸ’¡ æŠ€æœ¯åˆ›æ–°**
- [è¯†åˆ«æŠ€æœ¯æ–¹æ³•ä¸Šçš„åˆ›æ–°]

**ğŸ¯ åº”ç”¨åˆ›æ–°**
- [å‘ç°åº”ç”¨é¢†åŸŸçš„åˆ›æ–°]

**ğŸ“Š æ•°æ®åˆ›æ–°**
- [åˆ†ææ•°æ®å¤„ç†çš„åˆ›æ–°]

**ğŸ” ç†è®ºè´¡çŒ®**
- [æ€»ç»“ç†è®ºå±‚é¢çš„è´¡çŒ®]

**ğŸš€ å®ç”¨ä»·å€¼**
- [è¯„ä¼°å®é™…åº”ç”¨ä»·å€¼]

**ğŸŒŸ çªç ´æ€§å‘ç°**
- [è¯†åˆ«çªç ´æ€§çš„å‘ç°]

è¦æ±‚ï¼šæ·±åº¦æŒ–æ˜ï¼Œçªå‡ºåˆ›æ–°æ€§ï¼Œé¿å…æ³›æ³›è€Œè°ˆã€‚
"""
                        
                        headers = {
                            'Authorization': f'Bearer {api_token}',
                            'Content-Type': 'application/json'
                        }
                        
                        data = {
                            "model": selected_model,
                            "messages": [{"role": "user", "content": prompt}],
                            "stream": False,
                            "max_tokens": 1200,
                            "temperature": 0.8
                        }
                        
                        response = requests.post(
                            "https://api.siliconflow.cn/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=30
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            if 'choices' in result and len(result['choices']) > 0:
                                innovation = result['choices'][0]['message']['content']
                                
                                st.markdown("### ğŸ’¡ åˆ›æ–°ç‚¹åˆ†æ")
                                st.markdown(innovation)
                                
                                # ä¿å­˜ç»“æœ
                                if 'ai_summaries' not in st.session_state:
                                    st.session_state.ai_summaries = []
                                
                                st.session_state.ai_summaries.append({
                                    'summary': innovation,
                                    'model': selected_model,
                                    'type': 'åˆ›æ–°ç‚¹æŒ–æ˜',
                                    'input_count': len(abstracts_to_process),
                                    'timestamp': pd.Timestamp.now().isoformat()
                                })
                        
                except Exception as e:
                    st.error(f"åˆ›æ–°ç‚¹æŒ–æ˜å¤±è´¥: {str(e)}")
    
    elif not api_token:
        st.warning("âš ï¸ è¯·å…ˆé…ç½®API Tokenæ‰èƒ½ä½¿ç”¨AIåˆ†æåŠŸèƒ½")
    elif not selected_model:
        st.warning("âš ï¸ è¯·é€‰æ‹©æˆ–è¾“å…¥æ¨¡å‹åç§°")
    elif not abstracts_to_process:
        st.info("ğŸ’¡ è¯·å…ˆè¾“å…¥æ–‡çŒ®æ‘˜è¦å†…å®¹")
    
    # å†å²åˆ†æç»“æœ
    if 'ai_summaries' in st.session_state and st.session_state.ai_summaries:
        st.markdown("---")
        st.markdown("### ğŸ“š å†å²åˆ†æç»“æœ")
        
        # æŒ‰æ—¶é—´å€’åºæ˜¾ç¤º
        for i, summary_data in enumerate(reversed(st.session_state.ai_summaries), 1):
            with st.expander(f"ğŸ“‹ åˆ†æç»“æœ {i} - {summary_data['type']} ({summary_data['model']})"):
                st.markdown(f"**åˆ†ææ—¶é—´**: {summary_data['timestamp']}")
                st.markdown(f"**å¤„ç†æ–‡çŒ®æ•°**: {summary_data['input_count']}")
                st.markdown("**åˆ†æç»“æœ**:")
                st.markdown(summary_data['summary'])
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### ğŸ¤– AIæ‘˜è¦åŠŸèƒ½è¯´æ˜
        
        **è¾“å…¥æ–¹å¼**ï¼š
        - **å•ç¯‡æ‘˜è¦**: è¾“å…¥å•ç¯‡æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯
        - **æ‰¹é‡æ‘˜è¦**: ä¸€æ¬¡æ€§è¾“å…¥å¤šç¯‡æ‘˜è¦ï¼Œæ”¯æŒå¤šç§åˆ†éš”æ–¹å¼
        - **å‰ªè´´æ¿ç²˜è´´**: ç›´æ¥ç²˜è´´ä»å…¶ä»–åœ°æ–¹å¤åˆ¶çš„å†…å®¹
        
        **æ¨¡å‹é€‰æ‹©**ï¼š
        - **é¢„è®¾æ¨¡å‹**: ä»ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©é¢„é…ç½®çš„æ¨¡å‹
        - **è‡ªå®šä¹‰æ¨¡å‹**: æ‰‹åŠ¨è¾“å…¥å®Œæ•´çš„æ¨¡å‹åç§°
        - **æ”¯æŒæ¨¡å‹**: Qwen/QwQ-32B, Qwen/Qwen2.5-72B-Instruct, Qwen/Qwen3-8B ç­‰
        
        **åˆ†æç±»å‹**ï¼š
        - **æ ¸å¿ƒè¦ç‚¹æç‚¼**: å…¨é¢åˆ†ææ‘˜è¦ï¼Œæç‚¼æœ€é‡è¦çš„ä¿¡æ¯
        - **ç ”ç©¶æ–¹æ³•åˆ†æ**: é‡ç‚¹åˆ†æç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯è·¯çº¿
        - **åˆ›æ–°ç‚¹æŒ–æ˜**: æ·±åº¦æŒ–æ˜ç ”ç©¶çš„åˆ›æ–°æ€§å’Œçªç ´ç‚¹
        
        **ä½¿ç”¨æŠ€å·§**ï¼š
        - æ‘˜è¦å†…å®¹å»ºè®®åœ¨100-2000å­—ä¹‹é—´
        - æ”¯æŒä¸­è‹±æ–‡æ‘˜è¦ï¼ŒAIä¼šè‡ªåŠ¨ç”¨ä¸­æ–‡è¾“å‡º
        - æ‰¹é‡å¤„ç†æ—¶å»ºè®®ä¸è¶…è¿‡5ç¯‡æ‘˜è¦
        - å¯ä»¥å¤šæ¬¡ä½¿ç”¨ä¸åŒåˆ†æç±»å‹è·å¾—å…¨é¢æ´å¯Ÿ
        - è‡ªå®šä¹‰æ¨¡å‹æ—¶è¯·ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®
        
        **æ³¨æ„äº‹é¡¹**ï¼š
        - éœ€è¦æœ‰æ•ˆçš„ç¡…åŸºæµåŠ¨API Token
        - AIåˆ†æç»“æœä»…ä¾›å‚è€ƒï¼Œå»ºè®®ç»“åˆä¸“ä¸šåˆ¤æ–­
        - ä¸åŒæ¨¡å‹çš„åˆ†ææ•ˆæœå¯èƒ½æœ‰å·®å¼‚
        - è‡ªå®šä¹‰æ¨¡å‹éœ€è¦ç¡®ä¿åœ¨APIå¹³å°ä¸Šå¯ç”¨
        """)



# æ—§çš„æ¨¡æ‹Ÿå‡½æ•°å·²è¢«çœŸå®APIæ›¿ä»£



if __name__ == "__main__":
    main() 